"""
í™•ì¥ ê°€ëŠ¥í•œ í¬ë¡¤ë§ API ì„œë²„
- í”ŒëŸ¬ê·¸ì¸ êµ¬ì¡°ë¡œ ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ ì§€ì›
- í…”ë ˆê·¸ë¨ ë´‡ ì»¨íŠ¸ë¡¤
- ë´‡ ë°©ì§€ ê¸°ëŠ¥
"""

from flask import Flask, jsonify, request
import requests
from bs4 import BeautifulSoup
import os
import time
import random
from datetime import datetime
from abc import ABC, abstractmethod

app = Flask(__name__)

# ============================================================
# ğŸ“Œ ì„¤ì •
# ============================================================

API_SECRET_KEY = os.environ.get("API_SECRET_KEY", "default-secret-key-change-me")
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN", "")
ADMIN_CHAT_ID = os.environ.get("ADMIN_CHAT_ID", "")

# User-Agent ë¡œí…Œì´ì…˜
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
]

# í¬ë¡¤ëŸ¬ ìƒíƒœ (ë©”ëª¨ë¦¬ ì €ì¥, ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”)
CRAWLER_STATE = {
    "enabled": True,
    "galleries": ["thesingularity"]
}

# ============================================================
# ğŸ“Œ ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ (ì¶”ìƒ)
# ============================================================

class BaseCrawler(ABC):
    """ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
    
    def get_headers(self):
        return {
            "User-Agent": random.choice(USER_AGENTS),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
    
    def random_delay(self, min_sec=1, max_sec=3):
        """ë´‡ ë°©ì§€ìš© ëœë¤ ë”œë ˆì´"""
        time.sleep(random.uniform(min_sec, max_sec))
    
    @abstractmethod
    def get_list_url(self, gallery_id: str, page: int, recommend_only: bool) -> str:
        """ëª©ë¡ í˜ì´ì§€ URL ìƒì„±"""
        pass
    
    @abstractmethod
    def parse_list(self, html: str) -> list:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        pass
    
    @abstractmethod
    def get_detail_url(self, post_id: str, gallery_id: str) -> str:
        """ìƒì„¸ í˜ì´ì§€ URL ìƒì„±"""
        pass
    
    @abstractmethod
    def parse_detail(self, html: str) -> dict:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹± (ë³¸ë¬¸ + ì´ë¯¸ì§€)"""
        pass
    
    def crawl_list(self, gallery_id: str, page: int = 1, recommend_only: bool = True) -> dict:
        """ëª©ë¡ í¬ë¡¤ë§"""
        try:
            url = self.get_list_url(gallery_id, page, recommend_only)
            self.random_delay(0.5, 1.5)
            
            response = self.session.get(url, headers=self.get_headers(), timeout=10)
            response.raise_for_status()
            
            posts = self.parse_list(response.text)
            
            return {
                "success": True,
                "count": len(posts),
                "posts": posts,
                "crawledAt": datetime.now().isoformat()
            }
        except Exception as e:
            return {"success": False, "error": str(e), "posts": []}
    
    def crawl_detail(self, post_id: str, gallery_id: str) -> dict:
        """ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (ë³¸ë¬¸ + ì´ë¯¸ì§€)"""
        try:
            url = self.get_detail_url(post_id, gallery_id)
            self.random_delay(0.5, 1.5)
            
            response = self.session.get(url, headers=self.get_headers(), timeout=10)
            response.raise_for_status()
            
            detail = self.parse_detail(response.text)
            detail["success"] = True
            return detail
        except Exception as e:
            return {"success": False, "error": str(e)}


# ============================================================
# ğŸ“Œ ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ëŸ¬
# ============================================================

class DCInsideCrawler(BaseCrawler):
    """ë””ì‹œì¸ì‚¬ì´ë“œ ë§ˆì´ë„ˆê°¤ëŸ¬ë¦¬ í¬ë¡¤ëŸ¬"""
    
    def get_list_url(self, gallery_id: str, page: int, recommend_only: bool) -> str:
        base = f"https://gall.dcinside.com/mgallery/board/lists/?id={gallery_id}&page={page}"
        if recommend_only:
            base += "&exception_mode=recommend"
        return base
    
    def get_detail_url(self, post_id: str, gallery_id: str) -> str:
        return f"https://gall.dcinside.com/mgallery/board/view/?id={gallery_id}&no={post_id}"
    
    def parse_list(self, html: str) -> list:
        soup = BeautifulSoup(html, 'html.parser')
        posts = []
        rows = soup.select('tr.ub-content')
        
        for row in rows:
            try:
                post_id = row.get('data-no', '')
                if not post_id:
                    continue
                
                title_elem = row.select_one('td.gall_tit a')
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = title_elem.get('href', '')
                if link.startswith('/'):
                    link = f"https://gall.dcinside.com{link}"
                
                date_elem = row.select_one('td.gall_date')
                date = date_elem.get('title', '') or date_elem.get_text(strip=True) if date_elem else ''
                
                writer_elem = row.select_one('td.gall_writer')
                writer = writer_elem.get('data-nick', '') if writer_elem else ''
                
                count_elem = row.select_one('td.gall_count')
                view_count = count_elem.get_text(strip=True) if count_elem else ''
                
                recommend_elem = row.select_one('td.gall_recommend')
                recommend = recommend_elem.get_text(strip=True) if recommend_elem else ''
                
                posts.append({
                    'id': post_id,
                    'title': title,
                    'link': link,
                    'date': date,
                    'writer': writer,
                    'viewCount': view_count,
                    'recommend': recommend
                })
            except Exception:
                continue
        
        return posts
    
    def parse_detail(self, html: str) -> dict:
        soup = BeautifulSoup(html, 'html.parser')
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content_elem = soup.select_one('div.write_div')
        content = ""
        if content_elem:
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            content = content_elem.get_text(separator='\n', strip=True)
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        images = []
        if content_elem:
            for img in content_elem.select('img'):
                src = img.get('src', '')
                if src and 'dcimg' in src:
                    if src.startswith('//'):
                        src = 'https:' + src
                    images.append(src)
        
        return {
            "content": content[:5000],  # ìµœëŒ€ 5000ì
            "images": images[:10]  # ìµœëŒ€ 10ê°œ
        }


# ============================================================
# ğŸ“Œ í¬ë¡¤ëŸ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬
# ============================================================

CRAWLERS = {
    "dcinside": DCInsideCrawler
}

def get_crawler(site: str) -> BaseCrawler:
    """ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    crawler_class = CRAWLERS.get(site)
    if not crawler_class:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site}")
    return crawler_class()


# ============================================================
# ğŸ“Œ API Key ì¸ì¦
# ============================================================

def verify_api_key():
    provided_key = request.headers.get("X-API-Key") or request.args.get("api_key")
    if not provided_key:
        return False, "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤"
    if provided_key != API_SECRET_KEY:
        return False, "ì˜ëª»ëœ API Keyì…ë‹ˆë‹¤"
    return True, None


# ============================================================
# ğŸ“Œ í…”ë ˆê·¸ë¨ ìœ í‹¸
# ============================================================

def send_telegram_message(chat_id: str, text: str):
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡"""
    if not TELEGRAM_BOT_TOKEN:
        return False
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        requests.post(url, json={
            "chat_id": chat_id,
            "text": text,
            "parse_mode": "HTML"
        }, timeout=10)
        return True
    except:
        return False


# ============================================================
# ğŸ“Œ API ì—”ë“œí¬ì¸íŠ¸
# ============================================================

@app.route('/')
def home():
    return jsonify({
        'status': 'ok',
        'message': 'í™•ì¥ ê°€ëŠ¥ í¬ë¡¤ë§ API ì„œë²„',
        'version': '2.0',
        'supported_sites': list(CRAWLERS.keys()),
        'endpoints': {
            '/crawl': 'GET - ëª©ë¡ í¬ë¡¤ë§',
            '/crawl-detail': 'GET - ìƒì„¸ í¬ë¡¤ë§ (ë³¸ë¬¸+ì´ë¯¸ì§€)',
            '/status': 'GET - ìƒíƒœ í™•ì¸',
            '/health': 'GET - í—¬ìŠ¤ì²´í¬',
            '/webhook': 'POST - í…”ë ˆê·¸ë¨ Webhook'
        }
    })

@app.route('/health')
def health():
    return jsonify({'status': 'healthy', 'timestamp': datetime.now().isoformat()})

@app.route('/status')
def status():
    is_valid, error = verify_api_key()
    if not is_valid:
        return jsonify({'success': False, 'error': error}), 401
    
    return jsonify({
        'success': True,
        'enabled': CRAWLER_STATE['enabled'],
        'galleries': CRAWLER_STATE['galleries'],
        'supported_sites': list(CRAWLERS.keys())
    })

@app.route('/crawl')
def crawl():
    is_valid, error = verify_api_key()
    if not is_valid:
        return jsonify({'success': False, 'error': error}), 401
    
    if not CRAWLER_STATE['enabled']:
        return jsonify({'success': False, 'error': 'í¬ë¡¤ëŸ¬ê°€ ì¼ì‹œì •ì§€ ìƒíƒœì…ë‹ˆë‹¤'})
    
    site = request.args.get('site', 'dcinside')
    gallery_id = request.args.get('gallery_id', 'thesingularity')
    page = request.args.get('page', 1, type=int)
    recommend_only = request.args.get('recommend_only', 'true').lower() == 'true'
    
    try:
        crawler = get_crawler(site)
        result = crawler.crawl_list(gallery_id, page, recommend_only)
        result['site'] = site
        result['gallery_id'] = gallery_id
        return jsonify(result)
    except ValueError as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/crawl-detail')
def crawl_detail():
    is_valid, error = verify_api_key()
    if not is_valid:
        return jsonify({'success': False, 'error': error}), 401
    
    site = request.args.get('site', 'dcinside')
    gallery_id = request.args.get('gallery_id', 'thesingularity')
    post_id = request.args.get('post_id', '')
    
    if not post_id:
        return jsonify({'success': False, 'error': 'post_idê°€ í•„ìš”í•©ë‹ˆë‹¤'})
    
    try:
        crawler = get_crawler(site)
        result = crawler.crawl_detail(post_id, gallery_id)
        return jsonify(result)
    except ValueError as e:
        return jsonify({'success': False, 'error': str(e)})


# ============================================================
# ğŸ“Œ í…”ë ˆê·¸ë¨ Webhook
# ============================================================

@app.route('/webhook', methods=['POST'])
def telegram_webhook():
    """í…”ë ˆê·¸ë¨ ëª…ë ¹ì–´ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        message = data.get('message', {})
        chat_id = str(message.get('chat', {}).get('id', ''))
        text = message.get('text', '').strip()
        
        # ê´€ë¦¬ì ì²´í¬ (ì„ íƒì‚¬í•­)
        # if chat_id != ADMIN_CHAT_ID:
        #     return jsonify({'ok': True})
        
        if text == '/status':
            status_text = f"ğŸ¤– <b>í¬ë¡¤ëŸ¬ ìƒíƒœ</b>\n\n"
            status_text += f"ìƒíƒœ: {'âœ… ì‹¤í–‰ì¤‘' if CRAWLER_STATE['enabled'] else 'â¸ï¸ ì¼ì‹œì •ì§€'}\n"
            status_text += f"ê°¤ëŸ¬ë¦¬: {', '.join(CRAWLER_STATE['galleries'])}\n"
            status_text += f"ì§€ì› ì‚¬ì´íŠ¸: {', '.join(CRAWLERS.keys())}"
            send_telegram_message(chat_id, status_text)
        
        elif text == '/galleries':
            gall_text = "ğŸ“ <b>ê°¤ëŸ¬ë¦¬ ëª©ë¡</b>\n\n"
            for i, g in enumerate(CRAWLER_STATE['galleries'], 1):
                gall_text += f"{i}. {g}\n"
            send_telegram_message(chat_id, gall_text)
        
        elif text.startswith('/add '):
            gallery_id = text[5:].strip()
            if gallery_id and gallery_id not in CRAWLER_STATE['galleries']:
                CRAWLER_STATE['galleries'].append(gallery_id)
                send_telegram_message(chat_id, f"âœ… ê°¤ëŸ¬ë¦¬ ì¶”ê°€ë¨: {gallery_id}")
            else:
                send_telegram_message(chat_id, "âŒ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜ ì˜ëª»ëœ IDì…ë‹ˆë‹¤")
        
        elif text.startswith('/remove '):
            gallery_id = text[8:].strip()
            if gallery_id in CRAWLER_STATE['galleries']:
                CRAWLER_STATE['galleries'].remove(gallery_id)
                send_telegram_message(chat_id, f"âœ… ê°¤ëŸ¬ë¦¬ ì œê±°ë¨: {gallery_id}")
            else:
                send_telegram_message(chat_id, "âŒ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°¤ëŸ¬ë¦¬ì…ë‹ˆë‹¤")
        
        elif text == '/pause':
            CRAWLER_STATE['enabled'] = False
            send_telegram_message(chat_id, "â¸ï¸ í¬ë¡¤ë§ ì¼ì‹œì •ì§€ë¨")
        
        elif text == '/resume':
            CRAWLER_STATE['enabled'] = True
            send_telegram_message(chat_id, "â–¶ï¸ í¬ë¡¤ë§ ì¬ê°œë¨")
        
        elif text == '/help':
            help_text = """ğŸ¤– <b>ëª…ë ¹ì–´ ëª©ë¡</b>

/status - í˜„ì¬ ìƒíƒœ
/galleries - ê°¤ëŸ¬ë¦¬ ëª©ë¡
/add [ID] - ê°¤ëŸ¬ë¦¬ ì¶”ê°€
/remove [ID] - ê°¤ëŸ¬ë¦¬ ì œê±°
/pause - í¬ë¡¤ë§ ì¼ì‹œì •ì§€
/resume - í¬ë¡¤ë§ ì¬ê°œ
/help - ë„ì›€ë§"""
            send_telegram_message(chat_id, help_text)
        
        return jsonify({'ok': True})
    
    except Exception as e:
        print(f"Webhook error: {e}")
        return jsonify({'ok': True})


# ============================================================
# ğŸ“Œ ì‹¤í–‰
# ============================================================

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
